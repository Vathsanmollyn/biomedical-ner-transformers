{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ed2a045",
   "metadata": {},
   "source": [
    "\n",
    "# Fine-tuning PubMedBERT on MTL-Bioinformatics-2016 with Sentence Splitting\n",
    "\n",
    "This notebook:\n",
    "- Loads datasets (`combined_train.jsonl`, `combined_dev.jsonl`, `combined_test.jsonl`)\n",
    "- Splits long sequences ( > 512 tokens) into smaller chunks before tokenization\n",
    "- Fine-tunes BioBERT using Hugging Face Transformers\n",
    "- Uses `Trainer` API for efficient training and evaluation\n",
    "\n",
    "This fix prevents long sequence errors by ensuring that no input exceeds BioBERT's max length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50b37af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "import numpy as np\n",
    "from seqeval.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Load BioBERT model\n",
    "model_checkpoint = \"dmis-lab/biobert-base-cased-v1.2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "MAX_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92375362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train examples: 153823\n",
      "Dev examples: 58785\n",
      "Test examples: 99976\n"
     ]
    }
   ],
   "source": [
    "def load_jsonl(filepath):\n",
    "    data = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            data.append(json.loads(line.strip()))\n",
    "    return Dataset.from_list(data)\n",
    "\n",
    "train_dataset = load_jsonl('/media/smartdragon/WORK/6th Semester/22AIE315 - Natural Language Processing/Project/Batch 1/combined_train_1.jsonl')\n",
    "dev_dataset = load_jsonl('/media/smartdragon/WORK/6th Semester/22AIE315 - Natural Language Processing/Project/Batch 1/combined_dev_1.jsonl')\n",
    "test_dataset = load_jsonl('/media/smartdragon/WORK/6th Semester/22AIE315 - Natural Language Processing/Project/Batch 1/combined_test_1.jsonl')\n",
    "\n",
    "print(f\"Train examples: {len(train_dataset)}\")\n",
    "print(f\"Dev examples: {len(dev_dataset)}\")\n",
    "print(f\"Test examples: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98446a90-5802-4123-bab8-f8c5aa23c4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label2ID Mapping: {'B-Anatomy': 0, 'I-Anatomy': 1, 'O': 2}\n",
      "âœ… Saved label2id mapping.\n"
     ]
    }
   ],
   "source": [
    "# Generate label2id mapping\n",
    "unique_tags = set(tag for example in train_dataset['tags'] for tag in example)\n",
    "label2id = {tag: i for i, tag in enumerate(sorted(unique_tags))}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "print(f\"Label2ID Mapping: {label2id}\")\n",
    "\n",
    "# Save label2id to file\n",
    "with open(\"label2id.json\", \"w\") as f:\n",
    "    json.dump(label2id, f)\n",
    "print(\"âœ… Saved label2id mapping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df8945a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After splitting: Train=153823, Dev=58785, Test=99976\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def split_long_sentence(tokens, tags, max_len=MAX_LEN - 2):  # -2 for [CLS] and [SEP]\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), max_len):\n",
    "        chunk_tokens = tokens[i:i+max_len]\n",
    "        chunk_tags = tags[i:i+max_len]\n",
    "        chunks.append({'tokens': chunk_tokens, 'tags': chunk_tags})\n",
    "    return chunks\n",
    "\n",
    "# Apply sentence splitting before tokenization\n",
    "def preprocess_dataset(dataset):\n",
    "    split_data = []\n",
    "    for example in dataset:\n",
    "        split_sentences = split_long_sentence(example['tokens'], example['tags'])\n",
    "        split_data.extend(split_sentences)\n",
    "    return Dataset.from_list(split_data)\n",
    "\n",
    "train_dataset = preprocess_dataset(train_dataset)\n",
    "dev_dataset = preprocess_dataset(dev_dataset)\n",
    "test_dataset = preprocess_dataset(test_dataset)\n",
    "\n",
    "print(f\"After splitting: Train={len(train_dataset)}, Dev={len(dev_dataset)}, Test={len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c0a7489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label map: {'B-Anatomy': 0, 'I-Anatomy': 1, 'O': 2}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "843e1fef44224c868a933b3c85178e08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/153823 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "278a0615ba58434ca3b1e685f62e5162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/58785 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4fc6e7310e944cd85c3f6c4d96f4870",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/99976 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Extract label list from training set\n",
    "unique_tags = set(tag for example in train_dataset['tags'] for tag in example)\n",
    "label2id = {tag: i for i, tag in enumerate(sorted(unique_tags))}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "print(f\"Label map: {label2id}\")\n",
    "\n",
    "def tokenize_and_align_labels(example):\n",
    "    tokenized = tokenizer(example['tokens'], truncation=True, max_length=512, is_split_into_words=True)\n",
    "    word_ids = tokenized.word_ids()\n",
    "\n",
    "    aligned_labels = []\n",
    "    previous_word_idx = None\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            aligned_labels.append(-100)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            aligned_labels.append(label2id[example['tags'][word_idx]])\n",
    "        else:\n",
    "            aligned_labels.append(label2id[example['tags'][word_idx]])\n",
    "\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    tokenized['labels'] = aligned_labels\n",
    "    return tokenized\n",
    "\n",
    "# Tokenize datasets\n",
    "train_dataset = train_dataset.map(tokenize_and_align_labels)\n",
    "dev_dataset = dev_dataset.map(tokenize_and_align_labels)\n",
    "test_dataset = test_dataset.map(tokenize_and_align_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05b5f513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from transformers import AutoModelForTokenClassification\n",
    "\n",
    "# old_label2id = label2id\n",
    "# # Load the number of labels from batch 1\n",
    "# old_num_labels = len(old_label2id)  # Should match previous training setup\n",
    "# new_num_labels = len(label2id)  # Ensure this matches the new dataset\n",
    "\n",
    "# # Load the previously trained model from batch 1\n",
    "# model = AutoModelForTokenClassification.from_pretrained(\n",
    "#     model_path, num_labels=old_num_labels\n",
    "# )\n",
    "\n",
    "# # Extract the old classifier layer\n",
    "# old_classifier = model.classifier\n",
    "\n",
    "# # Create a new classifier layer with updated label count\n",
    "# new_classifier = nn.Linear(old_classifier.in_features, new_num_labels)\n",
    "\n",
    "# # Transfer weights from the old classifier to the new one (for common labels)\n",
    "# with torch.no_grad():\n",
    "#     num_common_labels = min(old_num_labels, new_num_labels)\n",
    "#     new_classifier.weight[:num_common_labels, :] = old_classifier.weight[:num_common_labels, :]\n",
    "#     new_classifier.bias[:num_common_labels] = old_classifier.bias[:num_common_labels]\n",
    "\n",
    "# # Assign the updated classifier to the model\n",
    "# model.classifier = new_classifier\n",
    "\n",
    "# # Save updated model before continuing training\n",
    "# model.save_pretrained(\"BioBERT-updated\")\n",
    "# print(\"âœ… Model updated to support new label set while keeping batch 1 training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caaa881c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/media/smartdragon/Storage/anaconda3/envs/nlp/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_5866/3549459300.py:60: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Adafactor\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label2id))\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"BioBERT-finetuned-mtl1\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    push_to_hub=False,\n",
    "    save_total_limit=2,\n",
    "    gradient_accumulation_steps=8,  # This simulates batch size 4 * 4 = 16\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "\n",
    "    # Convert predictions to label IDs\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_labels = []\n",
    "    true_predictions = []\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        label_sequence = []\n",
    "        prediction_sequence = []\n",
    "\n",
    "        for j in range(len(labels[i])):\n",
    "            if labels[i][j] != -100:  # Exclude padding tokens\n",
    "                label_sequence.append(id2label[labels[i][j]])\n",
    "                prediction_sequence.append(id2label[predictions[i][j]])\n",
    "\n",
    "        true_labels.append(label_sequence)\n",
    "        true_predictions.append(prediction_sequence)\n",
    "\n",
    "    # Classification report for F1, Precision, Recall\n",
    "    report = classification_report(true_labels, true_predictions, output_dict=True)\n",
    "\n",
    "    # Overall token-level accuracy\n",
    "    accuracy = accuracy_score(true_labels, true_predictions)\n",
    "\n",
    "    return {\n",
    "        \"f1\": report[\"micro avg\"][\"f1-score\"],\n",
    "        \"precision\": report[\"micro avg\"][\"precision\"],\n",
    "        \"recall\": report[\"micro avg\"][\"recall\"],\n",
    "        \"accuracy\": accuracy\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    # optimizers=(Adafactor(model.parameters(), scale_parameter=True, relative_step=True), None)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61407840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [600/600 04:57, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.147300</td>\n",
       "      <td>0.131489</td>\n",
       "      <td>0.708837</td>\n",
       "      <td>0.713054</td>\n",
       "      <td>0.704669</td>\n",
       "      <td>0.956488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=600, training_loss=0.14226804415384928, metrics={'train_runtime': 298.5404, 'train_samples_per_second': 1030.5, 'train_steps_per_second': 2.01, 'total_flos': 1079846168588748.0, 'train_loss': 0.14226804415384928, 'epoch': 1.995008319467554})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7c9cdaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1563' max='1563' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1563/1563 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Performance:\n",
      "{'eval_loss': 0.15424507856369019, 'eval_f1': 0.7271707785966036, 'eval_precision': 0.75311100049776, 'eval_recall': 0.7029580300449125, 'eval_accuracy': 0.9463610919322628, 'eval_runtime': 16.3671, 'eval_samples_per_second': 6108.354, 'eval_steps_per_second': 95.496, 'epoch': 1.995008319467554}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "metrics = trainer.evaluate(test_dataset)\n",
    "print(\"Test Set Performance:\")\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "321b2a57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('biobert-finetuned-batch1/tokenizer_config.json',\n",
       " 'biobert-finetuned-batch1/special_tokens_map.json',\n",
       " 'biobert-finetuned-batch1/vocab.txt',\n",
       " 'biobert-finetuned-batch1/added_tokens.json',\n",
       " 'biobert-finetuned-batch1/tokenizer.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"biobert-finetuned-batch1\")\n",
    "tokenizer.save_pretrained(\"biobert-finetuned-batch1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2206dde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved label2id mapping to model directory.\n"
     ]
    }
   ],
   "source": [
    "!mv label2id.json biobert-finetuned-batch1/label2id.json\n",
    "print(\"âœ… Saved label2id mapping to model directory.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
