{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Model Batch Fine-Tuning with Label\n",
    "This notebook implements a hybrid NER system that integrates transformer embeddings (SciBERT),\n",
    "external knowledge features from SciSpaCy, a BiLSTM layer (optional), and a CRF for sequence decoding.\n",
    "It supports batch-wise fine-tuning and dynamically updates the classifier and CRF layers when new labels appear.\n",
    "The code retains learned weights for base layers and handles device mismatches and optimizer state loading.\n",
    "To address CUDA out-of-memory issues, checkpoints are loaded on CPU and the batch size has been reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[1] - Environment Setup\n",
    "\n",
    "# Uncomment and run these if the packages are not yet installed:\n",
    "# !pip install torch torchvision torchaudio\n",
    "# !pip install transformers\n",
    "# !pip install scispacy\n",
    "# !pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/en_ner_bc5cdr_md-0.5.0.tar.gz\n",
    "# !pip install torchcrf\n",
    "# !pip install seqeval\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchcrf import CRF\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import spacy\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "from torch.utils.data.dataloader import default_collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[2] - GPU Check\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[3] - Load SciSpaCy Model for Knowledge Features\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load(\"en_ner_bc5cdr_md\")\n",
    "    print(\"Loaded SciSpaCy model: en_ner_bc5cdr_md\")\n",
    "except Exception as e:\n",
    "    nlp = None\n",
    "    print(\"Could not load SciSpaCy model. Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[4] - Define a Simple Knowledge Feature Extraction Function\n",
    "\n",
    "def get_knowledge_features(tokens):\n",
    "    \"\"\"\n",
    "    For each token in the sentence, return a binary feature (0 or 1)\n",
    "    indicating whether it is part of a recognized entity according to SciSpaCy.\n",
    "    \"\"\"\n",
    "    if nlp is None:\n",
    "        return [0] * len(tokens)\n",
    "    \n",
    "    text = \" \".join(tokens)\n",
    "    doc = nlp(text)\n",
    "    feats = [0] * len(tokens)\n",
    "    for ent in doc.ents:\n",
    "        for i in range(ent.start, ent.end):\n",
    "            if i < len(feats):\n",
    "                feats[i] = 1\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"allenai/scibert_scivocab_uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[6] - Define the NER Dataset Class\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, label2id, max_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label2id = label2id\n",
    "        self.max_length = max_length\n",
    "        self.samples = []\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                item = json.loads(line)\n",
    "                self.samples.append(item)\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.samples[idx]\n",
    "        tokens = item[\"tokens\"]\n",
    "        tags = item[\"tags\"]\n",
    "        knowledge_feats = get_knowledge_features(tokens)\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            tokens,\n",
    "            is_split_into_words=True,\n",
    "            return_tensors=\"pt\",\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_offsets_mapping=True\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].squeeze(0)\n",
    "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
    "        offset_mapping = encoding['offset_mapping'].squeeze(0)\n",
    "        \n",
    "        ner_ids = []\n",
    "        knowledge_ids = []\n",
    "        current_word_idx = 0\n",
    "        current_label = self.label2id[\"O\"]  # default label\n",
    "        \n",
    "        for i, offsets in enumerate(offset_mapping):\n",
    "            if offsets[0] == 0 and offsets[1] != 0:\n",
    "                if current_word_idx < len(tags):\n",
    "                    current_label = self.label2id.get(tags[current_word_idx], self.label2id[\"O\"])\n",
    "                    ner_ids.append(current_label)\n",
    "                    knowledge_ids.append(knowledge_feats[current_word_idx])\n",
    "                else:\n",
    "                    ner_ids.append(self.label2id[\"O\"])\n",
    "                    knowledge_ids.append(0)\n",
    "                current_word_idx += 1\n",
    "            else:\n",
    "                ner_ids.append(current_label)\n",
    "                knowledge_ids.append(knowledge_feats[current_word_idx-1] if current_word_idx > 0 else 0)\n",
    "                \n",
    "        ner_ids = ner_ids[:self.max_length]\n",
    "        knowledge_ids = knowledge_ids[:self.max_length]\n",
    "        \n",
    "        ner_ids = torch.tensor(ner_ids, dtype=torch.long)\n",
    "        knowledge_ids = torch.tensor(knowledge_ids, dtype=torch.float)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'ner_labels': ner_ids,\n",
    "            'knowledge_feats': knowledge_ids\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[8] - Define the NER Model\n",
    "\n",
    "class HybridNERModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 transformer_name=model_name, \n",
    "                 hidden_dim=128,\n",
    "                 num_ner_labels=10,\n",
    "                 knowledge_feature_dim=1,\n",
    "                 use_bilstm=True):\n",
    "        super(HybridNERModel, self).__init__()\n",
    "\n",
    "        self.transformer = AutoModel.from_pretrained(transformer_name)\n",
    "        transformer_hidden_size = self.transformer.config.hidden_size\n",
    "        self.feature_dim = transformer_hidden_size + knowledge_feature_dim\n",
    "\n",
    "        self.use_bilstm = use_bilstm\n",
    "        self.hidden_dim = hidden_dim\n",
    "        if self.use_bilstm:\n",
    "            self.bilstm = nn.LSTM(\n",
    "                input_size=self.feature_dim,\n",
    "                hidden_size=self.hidden_dim,\n",
    "                batch_first=True,\n",
    "                bidirectional=True\n",
    "            )\n",
    "            lstm_out_dim = self.hidden_dim * 2\n",
    "        else:\n",
    "            lstm_out_dim = self.feature_dim\n",
    "\n",
    "        self.num_ner_labels = num_ner_labels\n",
    "        self.ner_classifier = nn.Linear(lstm_out_dim, self.num_ner_labels)\n",
    "        self.crf = CRF(self.num_ner_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, knowledge_features, ner_labels=None):\n",
    "        outputs = self.transformer(input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "        if knowledge_features.dim() == 2:\n",
    "            knowledge_features = knowledge_features.unsqueeze(-1)\n",
    "        elif knowledge_features.dim() == 4:\n",
    "            knowledge_features = knowledge_features.squeeze(-1)\n",
    "\n",
    "        combined_input = torch.cat([last_hidden_state, knowledge_features], dim=-1)\n",
    "\n",
    "        if self.use_bilstm:\n",
    "            lstm_out, _ = self.bilstm(combined_input)\n",
    "        else:\n",
    "            lstm_out = combined_input\n",
    "\n",
    "        emissions = self.ner_classifier(lstm_out)\n",
    "\n",
    "        ner_loss = None\n",
    "        if ner_labels is not None:\n",
    "            if ner_labels.dim() == 3:\n",
    "                ner_labels = ner_labels.squeeze(1)\n",
    "            elif ner_labels.dim() == 1:\n",
    "                ner_labels = ner_labels.unsqueeze(1).expand(-1, emissions.shape[1])\n",
    "            emissions_t = emissions.transpose(0, 1)\n",
    "            labels_t = ner_labels.transpose(0, 1)\n",
    "            mask_t = attention_mask.bool().transpose(0, 1)\n",
    "            ner_loss = -1 * self.crf(emissions_t, labels_t, mask=mask_t)\n",
    "        return emissions, ner_loss\n",
    "\n",
    "    def decode(self, emissions, attention_mask):\n",
    "        emissions_t = emissions.transpose(0, 1)\n",
    "        mask_t = attention_mask.bool().transpose(0, 1)\n",
    "        pred_sequences = self.crf.decode(emissions_t, mask=mask_t)\n",
    "        return pred_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[9] - Define Training and Evaluation Functions\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_tokens = 0\n",
    "    for batch in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        ner_labels = batch['ner_labels'].to(device)\n",
    "        knowledge_feats = batch['knowledge_feats'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        emissions, ner_loss = model(input_ids, attention_mask, knowledge_feats, ner_labels=ner_labels)\n",
    "        if ner_loss is None:\n",
    "            continue\n",
    "        ner_loss = ner_loss.mean()\n",
    "        ner_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += ner_loss.item()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pred_sequences = model.decode(emissions, attention_mask)\n",
    "            for preds, golds, mask in zip(pred_sequences, ner_labels, attention_mask):\n",
    "                valid_len = mask.sum().item()\n",
    "                preds = preds[:valid_len]\n",
    "                golds = golds[:valid_len]\n",
    "                preds_tensor = torch.tensor(preds, device=golds.device)\n",
    "                correct = (preds_tensor == golds).sum().item()\n",
    "                total_correct += correct\n",
    "                total_tokens += valid_len\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = total_correct / total_tokens if total_tokens > 0 else 0.0\n",
    "    torch.cuda.empty_cache()\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluate(model, dataloader, id2label):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_true = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            ner_labels = batch['ner_labels'].to(device)\n",
    "            knowledge_feats = batch['knowledge_feats'].to(device)\n",
    "            \n",
    "            emissions, _ = model(input_ids, attention_mask, knowledge_feats, ner_labels=None)\n",
    "            pred_sequences = model.decode(emissions, attention_mask)\n",
    "            \n",
    "            for preds, golds, mask in zip(pred_sequences, ner_labels, attention_mask):\n",
    "                valid_len = mask.sum().item()\n",
    "                preds = preds[:valid_len]\n",
    "                golds = golds[:valid_len].cpu().numpy()\n",
    "                pred_labels = [id2label[p] for p in preds]\n",
    "                gold_labels = [id2label[g] if g != -100 else \"O\" for g in golds]\n",
    "                all_preds.append(pred_labels)\n",
    "                all_true.append(gold_labels)\n",
    "    print(\"SeqEval Classification Report:\")\n",
    "    print(classification_report(all_true, all_preds))\n",
    "    p = precision_score(all_true, all_preds)\n",
    "    r = recall_score(all_true, all_preds)\n",
    "    f1 = f1_score(all_true, all_preds)\n",
    "    print(f\"Precision: {p:.4f}, Recall: {r:.4f}, F1: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_for_finetuning(model, optimizer, checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    print(f\"Resuming training from epoch {start_epoch}\")\n",
    "    return start_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    batch_dict = {\n",
    "        \"input_ids\": torch.stack([item[\"input_ids\"] for item in batch]),\n",
    "        \"attention_mask\": torch.stack([item[\"attention_mask\"] for item in batch]),\n",
    "        \"ner_labels\": torch.stack([torch.tensor(item[\"ner_labels\"]) for item in batch]).squeeze(1),\n",
    "        \"knowledge_feats\": torch.stack([item[\"knowledge_feats\"] for item in batch])\n",
    "    }\n",
    "    return batch_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_previous_label2id(checkpoint_path):\n",
    "    label2id_path = os.path.join(checkpoint_path, \"label2id.json\")\n",
    "    if os.path.exists(label2id_path):\n",
    "        with open(label2id_path, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model_classifier(model, old_label2id, new_label2id):\n",
    "    old_num_labels = len(old_label2id) if old_label2id else 0\n",
    "    new_num_labels = len(new_label2id)\n",
    "    old_classifier = model.ner_classifier\n",
    "    new_classifier = nn.Linear(old_classifier.in_features, new_num_labels)\n",
    "    with torch.no_grad():\n",
    "        num_common_labels = min(old_num_labels, new_num_labels)\n",
    "        new_classifier.weight[:num_common_labels, :] = old_classifier.weight[:num_common_labels, :]\n",
    "        new_classifier.bias[:num_common_labels] = old_classifier.bias[:num_common_labels]\n",
    "    model.ner_classifier = new_classifier\n",
    "    model.crf = CRF(new_num_labels)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def natural_sort_key(file_name):\n",
    "    numbers = re.findall(r'\\d+', file_name)\n",
    "    return [int(num) for num in numbers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/smartdragon/Storage/anaconda3/envs/nlp/lib/python3.12/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_ner_bc5cdr_md' (0.5.4) was trained with spaCy v3.7.4 and may not be 100% compatible with the current version (3.8.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/media/smartdragon/Storage/anaconda3/envs/nlp/lib/python3.12/site-packages/spacy/language.py:2232: FutureWarning: Possible set union at position 6328\n",
      "  deserializers[\"tokenizer\"] = lambda p: self.tokenizer.from_disk(  # type: ignore[union-attr]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded SciSpaCy model: en_ner_bc5cdr_md\n",
      "✅ Correctly sorted 19 training batches, 19 dev batches, 19 test batches\n",
      "Found 19 training batches, 19 dev batches, 19 test batches\n",
      "🚀 Processing batch 2\n",
      "✅ Saved label2id for batch 2\n",
      "Looking for model checkpoint: /media/smartdragon/Windows-SSD/Users/sriva/Documents/NLP/HybridModel/batch_1/model.pt\n",
      "✅ Loaded model checkpoint for batch 2 (filtered classifier and CRF parameters)\n",
      "✅ Updated model classifier for batch 2\n",
      "⚠️ Skipping optimizer state load due to classifier update (label mapping change)\n",
      "✅ Successfully set up processing for batch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/9614 [00:00<?, ?it/s]/tmp/ipykernel_119628/57700023.py:297: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"ner_labels\": torch.stack([torch.tensor(item[\"ner_labels\"]) for item in batch]).squeeze(1),\n",
      "Training:  11%|█         | 1051/9614 [01:45<15:07,  9.44it/s]"
     ]
    }
   ],
   "source": [
    "# Define directory containing batch files\n",
    "batch_data_dir = \"/media/smartdragon/WORK/6th Semester/22AIE315 - Natural Language Processing/Project/New_Json_Files\"  # Update with your actual path\n",
    "\n",
    "train_files = sorted(glob.glob(os.path.join(batch_data_dir, \"combined_train_*.jsonl\")), key=natural_sort_key)\n",
    "dev_files = sorted(glob.glob(os.path.join(batch_data_dir, \"combined_dev_*.jsonl\")), key=natural_sort_key)\n",
    "test_files = sorted(glob.glob(os.path.join(batch_data_dir, \"combined_test_*.jsonl\")), key=natural_sort_key)\n",
    "\n",
    "print(f\"✅ Correctly sorted {len(train_files)} training batches, {len(dev_files)} dev batches, {len(test_files)} test batches\")\n",
    "print(f\"Found {len(train_files)} training batches, {len(dev_files)} dev batches, {len(test_files)} test batches\")\n",
    "\n",
    "checkpoint_path = \"/media/smartdragon/Windows-SSD/Users/sriva/Documents/NLP/HybridModel\"\n",
    "os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
    "base_checkpoint_path = \"/media/smartdragon/Windows-SSD/Users/sriva/Documents/NLP/HybridModel\"\n",
    "\n",
    "# %% [code]\n",
    "# Iterate over each batch and fine-tune sequentially\n",
    "for batch_idx, train_file in enumerate(train_files):\n",
    "    batch_number = train_file.split(\"_\")[-1].split(\".\")[0]\n",
    "    \n",
    "    old_checkpoint_path = os.path.join(base_checkpoint_path, f\"batch_{int(batch_number)-1}\")\n",
    "    checkpoint_path = os.path.join(base_checkpoint_path, f\"batch_{batch_number}\")\n",
    "    print(f\"🚀 Processing batch {batch_number}\")\n",
    "    \n",
    "    os.makedirs(checkpoint_path, exist_ok=True)\n",
    "    \n",
    "    previous_label2id = load_previous_label2id(old_checkpoint_path)\n",
    "    \n",
    "    with open(train_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw_data = [json.loads(line.strip()) for line in f]\n",
    "    \n",
    "    unique_tags = set(tag for example in raw_data for tag in example['tags'])\n",
    "    label2id = {tag: i for i, tag in enumerate(sorted(unique_tags))}\n",
    "    \n",
    "    label2id_path = os.path.join(checkpoint_path, \"label2id.json\")\n",
    "    with open(label2id_path, \"w\") as f:\n",
    "        json.dump(label2id, f)\n",
    "    print(f\"✅ Saved label2id for batch {batch_number}\")\n",
    "    \n",
    "    model = HybridNERModel(\n",
    "        transformer_name=model_name,\n",
    "        hidden_dim=128,\n",
    "        num_ner_labels=len(label2id),\n",
    "        knowledge_feature_dim=1,\n",
    "        use_bilstm=True\n",
    "    ).to(device)\n",
    "    \n",
    "    model_checkpoint = os.path.join(old_checkpoint_path, \"model.pt\")\n",
    "    print(f\"Looking for model checkpoint: {model_checkpoint}\")\n",
    "    \n",
    "    if os.path.exists(model_checkpoint):\n",
    "        checkpoint = torch.load(model_checkpoint, map_location=\"cpu\")\n",
    "        filtered_state_dict = {k: v for k, v in checkpoint[\"model_state_dict\"].items()\n",
    "                                if not (k.startswith(\"ner_classifier\") or k.startswith(\"crf\"))}\n",
    "        model.load_state_dict(filtered_state_dict, strict=False)\n",
    "        print(f\"✅ Loaded model checkpoint for batch {batch_number} (filtered classifier and CRF parameters)\")\n",
    "    \n",
    "    if previous_label2id and previous_label2id != label2id:\n",
    "        model = update_model_classifier(model, previous_label2id, label2id)\n",
    "        model = model.to(device)\n",
    "        print(f\"✅ Updated model classifier for batch {batch_number}\")\n",
    "    \n",
    "    # Use a smaller batch size to reduce GPU memory usage\n",
    "    train_dataset = NERDataset(train_file, tokenizer, label2id, max_length=4)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=custom_collate_fn)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    \n",
    "    if os.path.exists(model_checkpoint) and (previous_label2id is None or previous_label2id == label2id):\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        print(f\"✅ Loaded optimizer state for batch {batch_number}\")\n",
    "    else:\n",
    "        print(\"⚠️ Skipping optimizer state load due to classifier update (label mapping change)\")\n",
    "    \n",
    "    print(f\"✅ Successfully set up processing for batch {batch_number}\")\n",
    "    \n",
    "    num_finetune_epochs = 1\n",
    "    for epoch in range(num_finetune_epochs):\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer)\n",
    "        print(f\"Batch {batch_number}, Epoch {epoch+1}, Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}\")\n",
    "    \n",
    "    new_checkpoint_path = os.path.join(checkpoint_path, \"model.pt\")\n",
    "    torch.save({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"loss\": train_loss,\n",
    "    }, new_checkpoint_path)\n",
    "    print(f\"✅ Saved model checkpoint for batch {batch_number}\")\n",
    "\n",
    "print(\"🎉 All batches processed successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
