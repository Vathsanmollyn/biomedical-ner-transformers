{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11fccc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[1] - Environment Setup\n",
    "\n",
    "# Uncomment and run these if the packages are not yet installed:\n",
    "# !pip install torch torchvision torchaudio\n",
    "# !pip install transformers\n",
    "# !pip install scispacy\n",
    "# !pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/en_ner_bc5cdr_md-0.5.0.tar.gz\n",
    "# !pip install torchcrf\n",
    "# !pip install seqeval\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchcrf import CRF\n",
    "# from datasets import Dataset\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import spacy\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "997706f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# In[2] - GPU Check\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ca7f9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/smartdragon/Storage/anaconda3/envs/nlp/lib/python3.12/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_ner_bc5cdr_md' (0.5.4) was trained with spaCy v3.7.4 and may not be 100% compatible with the current version (3.8.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded SciSpaCy model: en_ner_bc5cdr_md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/smartdragon/Storage/anaconda3/envs/nlp/lib/python3.12/site-packages/spacy/language.py:2232: FutureWarning: Possible set union at position 6328\n",
      "  deserializers[\"tokenizer\"] = lambda p: self.tokenizer.from_disk(  # type: ignore[union-attr]\n"
     ]
    }
   ],
   "source": [
    "# In[3] - Load SciSpaCy Model for Knowledge Features\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load(\"en_ner_bc5cdr_md\")\n",
    "    print(\"Loaded SciSpaCy model: en_ner_bc5cdr_md\")\n",
    "except Exception as e:\n",
    "    nlp = None\n",
    "    print(\"Could not load SciSpaCy model. Error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ddaca8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[4] - Define a Simple Knowledge Feature Extraction Function\n",
    "\n",
    "def get_knowledge_features(tokens):\n",
    "    \"\"\"\n",
    "    For each token in the sentence, return a binary feature (0 or 1)\n",
    "    indicating whether it is part of a recognized entity according to SciSpaCy.\n",
    "    \n",
    "    This is a simple placeholder; you can extend it to incorporate UMLS concept IDs\n",
    "    or richer biomedical features.\n",
    "    \"\"\"\n",
    "    if nlp is None:\n",
    "        return [0] * len(tokens)\n",
    "    \n",
    "    text = \" \".join(tokens)\n",
    "    doc = nlp(text)\n",
    "    feats = [0] * len(tokens)\n",
    "    for ent in doc.ents:\n",
    "        # Mark each token within the entity span as 1\n",
    "        for i in range(ent.start, ent.end):\n",
    "            if i < len(feats):\n",
    "                feats[i] = 1\n",
    "    return feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5596224f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def load_jsonl(filepath):\n",
    "#     data = []\n",
    "#     with open(filepath, 'r', encoding='utf-8') as file:\n",
    "#         for line in file:\n",
    "#             data.append(json.loads(line.strip()))\n",
    "#     return Dataset.from_list(data)\n",
    "\n",
    "# # Load datasets\n",
    "# train_dataset = load_jsonl('/media/smartdragon/WORK/6th Semester/22AIE315 - Natural Language Processing/Project/Batch 1/combined_train_1.jsonl')\n",
    "# dev_dataset = load_jsonl('/media/smartdragon/WORK/6th Semester/22AIE315 - Natural Language Processing/Project/Batch 1/combined_dev_1.jsonl')\n",
    "# test_dataset = load_jsonl('/media/smartdragon/WORK/6th Semester/22AIE315 - Natural Language Processing/Project/Batch 1/combined_test_1.jsonl')\n",
    "\n",
    "# print(f\"Train examples: {len(train_dataset)}\")\n",
    "# print(f\"Dev examples: {len(dev_dataset)}\")\n",
    "# print(f\"Test examples: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b18fc4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded label mapping with 3 labels.\n"
     ]
    }
   ],
   "source": [
    "# In[5] - Load Label Mapping from JSON\n",
    "\n",
    "# Update the file path if needed.\n",
    "with open(\"/media/smartdragon/WORK/6th Semester/22AIE315 - Natural Language Processing/Project/Hybrid Model/label2id.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    label2id = json.load(f)\n",
    "\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "print(\"Loaded label mapping with\", len(label2id), \"labels.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3a9ee0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[6] - Define the NER Dataset Class\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, label2id, max_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label2id = label2id\n",
    "        self.max_length = max_length\n",
    "        self.samples = []  # renamed to avoid conflict with any built-in properties\n",
    "        \n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            # Each line is a JSON object like: {\"tokens\": [...], \"tags\": [...]}\n",
    "            for line in f:\n",
    "                item = json.loads(line)\n",
    "                self.samples.append(item)\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.samples[idx]\n",
    "        tokens = item[\"tokens\"]\n",
    "        tags = item[\"tags\"]\n",
    "        \n",
    "        # Get knowledge features (a list of 0/1 values)\n",
    "        knowledge_feats = get_knowledge_features(tokens)\n",
    "        \n",
    "        # Tokenize using the Hugging Face tokenizer\n",
    "        encoding = self.tokenizer(\n",
    "            tokens,\n",
    "            is_split_into_words=True,\n",
    "            return_tensors=\"pt\",\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_offsets_mapping=True\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].squeeze(0)         # (max_length,)\n",
    "        attention_mask = encoding['attention_mask'].squeeze(0)   # (max_length,)\n",
    "        offset_mapping = encoding['offset_mapping'].squeeze(0)   # (max_length, 2)\n",
    "        \n",
    "        ner_ids = []\n",
    "        knowledge_ids = []\n",
    "        current_word_idx = 0\n",
    "        current_label = self.label2id[\"O\"]  # default label\n",
    "\n",
    "        # Align the labels with the subword tokens:\n",
    "        for i, offsets in enumerate(offset_mapping):\n",
    "            # Check if the token is the first subword of a word.\n",
    "            if offsets[0] == 0 and offsets[1] != 0:\n",
    "                if current_word_idx < len(tags):\n",
    "                    current_label = self.label2id.get(tags[current_word_idx], self.label2id[\"O\"])\n",
    "                    ner_ids.append(current_label)\n",
    "                    knowledge_ids.append(knowledge_feats[current_word_idx])\n",
    "                else:\n",
    "                    ner_ids.append(self.label2id[\"O\"])\n",
    "                    knowledge_ids.append(0)\n",
    "                current_word_idx += 1\n",
    "            else:\n",
    "                # For subsequent subwords, replicate the label of the first subword.\n",
    "                ner_ids.append(current_label)\n",
    "                knowledge_ids.append(knowledge_feats[current_word_idx-1] if current_word_idx > 0 else 0)\n",
    "        \n",
    "        # Truncate lists if they exceed max_length\n",
    "        ner_ids = ner_ids[:self.max_length]\n",
    "        knowledge_ids = knowledge_ids[:self.max_length]\n",
    "        \n",
    "        ner_ids = torch.tensor(ner_ids, dtype=torch.long)\n",
    "        knowledge_ids = torch.tensor(knowledge_ids, dtype=torch.float)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'ner_labels': ner_ids,\n",
    "            'knowledge_feats': knowledge_ids\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23773ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets sizes: 153823 58785 99976\n"
     ]
    }
   ],
   "source": [
    "# In[7] - Instantiate Tokenizer and Create DataLoaders\n",
    "\n",
    "# Update the model name if you prefer PubMedBERT; here we use a Bio/clinical model.\n",
    "model_name = \"allenai/scibert_scivocab_uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Update file paths to your converted JSONL files.\n",
    "train_file = \"/media/smartdragon/WORK/6th Semester/22AIE315 - Natural Language Processing/Project/Batch 1/combined_train_1.jsonl\"\n",
    "dev_file   = \"/media/smartdragon/WORK/6th Semester/22AIE315 - Natural Language Processing/Project/Batch 1/combined_dev_1.jsonl\"\n",
    "test_file  = \"/media/smartdragon/WORK/6th Semester/22AIE315 - Natural Language Processing/Project/Batch 1/combined_test_1.jsonl\"\n",
    "\n",
    "train_dataset = NERDataset(train_file, tokenizer, label2id, max_length=4)\n",
    "dev_dataset   = NERDataset(dev_file, tokenizer, label2id, max_length=4)\n",
    "test_dataset  = NERDataset(test_file, tokenizer, label2id, max_length=4)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "dev_loader   = DataLoader(dev_dataset, batch_size=64, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(\"Datasets sizes:\", len(train_dataset), len(dev_dataset), len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebe613cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[8] - Define the Hybrid NER Model (Transformer + Optional BiLSTM + CRF)\n",
    "\n",
    "class HybridNERModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 transformer_name=model_name, \n",
    "                 hidden_dim=128,\n",
    "                 num_ner_labels=len(label2id),\n",
    "                 knowledge_feature_dim=1,\n",
    "                 use_bilstm=True):\n",
    "        super(HybridNERModel, self).__init__()\n",
    "        \n",
    "        # Transformer backbone\n",
    "        self.transformer = AutoModel.from_pretrained(transformer_name)\n",
    "        transformer_hidden_size = self.transformer.config.hidden_size\n",
    "        \n",
    "        # Combine transformer output with knowledge features\n",
    "        self.feature_dim = transformer_hidden_size + knowledge_feature_dim\n",
    "        \n",
    "        # BiLSTM layer (optional)\n",
    "        self.use_bilstm = use_bilstm\n",
    "        self.hidden_dim = hidden_dim\n",
    "        if self.use_bilstm:\n",
    "            self.bilstm = nn.LSTM(\n",
    "                input_size=self.feature_dim,\n",
    "                hidden_size=self.hidden_dim,\n",
    "                batch_first=True,\n",
    "                bidirectional=True\n",
    "            )\n",
    "            lstm_out_dim = self.hidden_dim * 2\n",
    "        else:\n",
    "            lstm_out_dim = self.feature_dim\n",
    "        \n",
    "        # CRF layer for NER\n",
    "        self.num_ner_labels = num_ner_labels\n",
    "        self.ner_classifier = nn.Linear(lstm_out_dim, self.num_ner_labels)\n",
    "        self.crf = CRF(self.num_ner_labels)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, knowledge_features, labels_ner=None):\n",
    "        outputs = self.transformer(input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state  # (B, T, H)\n",
    "        \n",
    "        # Ensure knowledge_features has shape (B, T, 1)\n",
    "        if len(knowledge_features.shape) == 2:\n",
    "            knowledge_features = knowledge_features.unsqueeze(-1)\n",
    "        \n",
    "        combined_input = torch.cat([last_hidden_state, knowledge_features], dim=-1)\n",
    "        \n",
    "        if self.use_bilstm:\n",
    "            lstm_out, _ = self.bilstm(combined_input)  # (B, T, 2*hidden_dim)\n",
    "        else:\n",
    "            lstm_out = combined_input\n",
    "        \n",
    "        emissions = self.ner_classifier(lstm_out)  # (B, T, num_ner_labels)\n",
    "        \n",
    "        ner_loss = None\n",
    "        if labels_ner is not None:\n",
    "            # CRF expects emissions of shape (T, B, num_labels), so transpose\n",
    "            emissions_t = emissions.transpose(0, 1)\n",
    "            labels_t = labels_ner.transpose(0, 1)\n",
    "            mask_t = attention_mask.bool().transpose(0, 1)\n",
    "            ner_loss = -1 * self.crf(emissions_t, labels_t, mask=mask_t)\n",
    "        \n",
    "        return emissions, ner_loss\n",
    "    \n",
    "    def decode(self, emissions, attention_mask):\n",
    "        # Transpose from (B, T, num_labels) to (T, B, num_labels)\n",
    "        emissions_t = emissions.transpose(0, 1)\n",
    "        # Transpose mask from (B, T) to (T, B)\n",
    "        mask_t = attention_mask.bool().transpose(0, 1)\n",
    "\n",
    "        # Use the CRF's decode method\n",
    "        pred_sequences = self.crf.decode(emissions_t, mask=mask_t)\n",
    "        return pred_sequences\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f6a4f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[9] - Define Training and Evaluation Functions\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    # Counters for token-level accuracy\n",
    "    total_correct = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        ner_labels = batch['ner_labels'].to(device)\n",
    "        knowledge_feats = batch['knowledge_feats'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        emissions, ner_loss = model(input_ids, attention_mask, knowledge_feats, labels_ner=ner_labels)\n",
    "        \n",
    "        # Some CRF implementations return a vector per batch element.\n",
    "        # Ensure we reduce to a scalar:\n",
    "        if ner_loss is None:\n",
    "            continue\n",
    "        ner_loss = ner_loss.mean()\n",
    "        \n",
    "        ner_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += ner_loss.item()\n",
    "        \n",
    "        # --- Compute token-level accuracy ---\n",
    "        with torch.no_grad():\n",
    "            # CRF decode => list of predicted label sequences\n",
    "            pred_sequences = model.decode(emissions, attention_mask)\n",
    "            \n",
    "            # Compare predictions to gold labels\n",
    "            for preds, golds, mask in zip(pred_sequences, ner_labels, attention_mask):\n",
    "                valid_len = mask.sum().item()  # number of real tokens\n",
    "                preds = preds[:valid_len]      # slice predictions to valid length\n",
    "                golds = golds[:valid_len]      # slice gold labels\n",
    "                \n",
    "                # Convert preds to a tensor (so we can do a direct == compare)\n",
    "                preds_tensor = torch.tensor(preds, device=golds.device)\n",
    "                \n",
    "                # Count how many are correct\n",
    "                correct = (preds_tensor == golds).sum().item()\n",
    "                total_correct += correct\n",
    "                total_tokens += valid_len\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = total_correct / total_tokens if total_tokens > 0 else 0.0\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, id2label):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_true = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            ner_labels = batch['ner_labels'].to(device)\n",
    "            knowledge_feats = batch['knowledge_feats'].to(device)\n",
    "            \n",
    "            emissions, _ = model(input_ids, attention_mask, knowledge_feats, labels_ner=None)\n",
    "            pred_sequences = model.decode(emissions, attention_mask)\n",
    "            \n",
    "            for preds, golds, mask in zip(pred_sequences, ner_labels, attention_mask):\n",
    "                valid_len = mask.sum().item()\n",
    "                preds = preds[:valid_len]\n",
    "                golds = golds[:valid_len].cpu().numpy()\n",
    "                pred_labels = [id2label[p] for p in preds]\n",
    "                gold_labels = [id2label[g] if g != -100 else \"O\" for g in golds]\n",
    "                all_preds.append(pred_labels)\n",
    "                all_true.append(gold_labels)\n",
    "    \n",
    "    print(\"SeqEval Classification Report:\")\n",
    "    print(classification_report(all_true, all_preds))\n",
    "    p = precision_score(all_true, all_preds)\n",
    "    r = recall_score(all_true, all_preds)\n",
    "    f1 = f1_score(all_true, all_preds)\n",
    "    print(f\"Precision: {p:.4f}, Recall: {r:.4f}, F1: {f1:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a529ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 14.1038, Train Accuracy: 0.9711\n",
      "Epoch 1, Train Loss: 14.1038\n",
      "Evaluation on Dev Set:\n",
      "SeqEval Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Anatomy       0.59      0.61      0.60      6015\n",
      "\n",
      "   micro avg       0.59      0.61      0.60      6015\n",
      "   macro avg       0.59      0.61      0.60      6015\n",
      "weighted avg       0.59      0.61      0.60      6015\n",
      "\n",
      "Precision: 0.5922, Recall: 0.6148, F1: 0.6033\n"
     ]
    }
   ],
   "source": [
    "# In[10] - Instantiate Model and Run Training Loop\n",
    "\n",
    "model = HybridNERModel(\n",
    "    transformer_name=model_name,\n",
    "    hidden_dim=128,\n",
    "    num_ner_labels=len(label2id),\n",
    "    knowledge_feature_dim=1,\n",
    "    use_bilstm=True\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer)\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n",
    "    print(\"Evaluation on Dev Set:\")\n",
    "    evaluate(model, dev_loader, id2label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24f2c221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Evaluation on Test Set:\n",
      "SeqEval Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Anatomy       0.65      0.62      0.63     13118\n",
      "\n",
      "   micro avg       0.65      0.62      0.63     13118\n",
      "   macro avg       0.65      0.62      0.63     13118\n",
      "weighted avg       0.65      0.62      0.63     13118\n",
      "\n",
      "Precision: 0.6496, Recall: 0.6199, F1: 0.6344\n"
     ]
    }
   ],
   "source": [
    "# In[11] - Final Evaluation on Test Set\n",
    "\n",
    "print(\"Final Evaluation on Test Set:\")\n",
    "evaluate(model, test_loader, id2label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b0d9683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model's state_dict and optimizer's state_dict\n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': train_loss,\n",
    "}, \"hybrid_ner_model_1.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
