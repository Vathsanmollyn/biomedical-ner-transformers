{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0178dce1",
   "metadata": {},
   "source": [
    "\n",
    "# MTL-Bioinformatics-2016 - Preparing Data for BioBERT Training\n",
    "\n",
    "This notebook automates the process of:\n",
    "- Loading all datasets from MTL-Bioinformatics-2016\n",
    "- Converting the data into a format compatible with Hugging Face Transformers\n",
    "- Creating train, dev, and test splits ready for BioBERT fine-tuning\n",
    "\n",
    "The datasets will be combined into a single training corpus if desired, or you can train on individual datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "78a620f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Base folder for datasets\n",
    "base_folder = '/media/smartdragon/WORK/6th Semester/22AIE315 - Natural Language Processing/Project/MTL-Bioinformatics-2016_1/data/67'\n",
    "\n",
    "\n",
    "# Function to read CoNLL files into sentences\n",
    "def read_conll_file(filepath):\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            if line.strip():\n",
    "                sentence.append(line.strip().split())\n",
    "            else:\n",
    "                if sentence:\n",
    "                    sentences.append(sentence)\n",
    "                    sentence = []\n",
    "    if sentence:\n",
    "        sentences.append(sentence)\n",
    "    return sentences\n",
    "\n",
    "# Convert sentences into Hugging Face-friendly format\n",
    "def convert_to_hf_format(sentences):\n",
    "    examples = []\n",
    "    for sentence in sentences:\n",
    "        tokens = [token for token, tag in sentence]\n",
    "        tags = [tag for token, tag in sentence]\n",
    "        examples.append({\"tokens\": tokens, \"tags\": tags})\n",
    "    return examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "92ae0ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NCBI-disease-IOBES - Train: 5424, Dev: 923, Test: 940\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Dictionary to hold all datasets' splits\n",
    "all_datasets = defaultdict(lambda: {\"train\": [], \"dev\": [], \"test\": []})\n",
    "\n",
    "# Loop through datasets in the data folder\n",
    "for dataset in os.listdir(base_folder):\n",
    "    dataset_folder = os.path.join(base_folder, dataset)\n",
    "    if not os.path.isdir(dataset_folder):\n",
    "        continue\n",
    "\n",
    "    train_file = os.path.join(dataset_folder, 'train.tsv')\n",
    "    test_file = os.path.join(dataset_folder, 'test.tsv')\n",
    "    dev_file = os.path.join(dataset_folder, 'devel.tsv')\n",
    "\n",
    "    train_data = read_conll_file(train_file) if os.path.exists(train_file) else []\n",
    "    test_data = read_conll_file(test_file) if os.path.exists(test_file) else []\n",
    "    dev_data = read_conll_file(dev_file) if os.path.exists(dev_file) else []\n",
    "\n",
    "    all_datasets[dataset][\"train\"] = convert_to_hf_format(train_data)\n",
    "    all_datasets[dataset][\"dev\"] = convert_to_hf_format(dev_data)\n",
    "    all_datasets[dataset][\"test\"] = convert_to_hf_format(test_data)\n",
    "\n",
    "    print(f\"{dataset} - Train: {len(train_data)}, Dev: {len(dev_data)}, Test: {len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bce4c76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset size - Train: 5424, Dev: 923, Test: 940\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Combine all datasets into a single training/dev/test set if desired\n",
    "combined_train = []\n",
    "combined_dev = []\n",
    "combined_test = []\n",
    "\n",
    "for dataset, splits in all_datasets.items():\n",
    "    combined_train.extend(splits['train'])\n",
    "    combined_dev.extend(splits['dev'])\n",
    "    combined_test.extend(splits['test'])\n",
    "\n",
    "print(f\"Combined dataset size - Train: {len(combined_train)}, Dev: {len(combined_dev)}, Test: {len(combined_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a854e4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved combined datasets for Hugging Face training.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "output_folder = '/media/smartdragon/WORK/6th Semester/22AIE315 - Natural Language Processing/Project/New_Json_Files'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "def save_jsonl(data, filepath):\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        for example in data:\n",
    "            f.write(json.dumps(example) + '\\n')\n",
    "\n",
    "save_jsonl(combined_train, os.path.join(output_folder, 'combined_train_67.jsonl'))\n",
    "save_jsonl(combined_dev, os.path.join(output_folder, 'combined_dev_67.jsonl'))\n",
    "save_jsonl(combined_test, os.path.join(output_folder, 'combined_test_67.jsonl'))\n",
    "\n",
    "print(\"Saved combined datasets for Hugging Face training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7f88400a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             tokens tags\n",
      "0  [Identification]  [O]\n",
      "1              [of]  [O]\n",
      "2            [APC2]  [O]\n",
      "3               [,]  [O]\n",
      "4               [a]  [O]\n"
     ]
    }
   ],
   "source": [
    "# Reattempting to process the dataset with one token per label\n",
    "\n",
    "# Load dataset again\n",
    "file_path = \"/media/smartdragon/WORK/6th Semester/22AIE315 - Natural Language Processing/Project/New_Json_Files/combined_train_67.jsonl\"\n",
    "\n",
    "# Read JSONL file into a list\n",
    "data = []\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        data.append(json.loads(line.strip()))\n",
    "\n",
    "# Convert to one token per label\n",
    "processed_data = []\n",
    "for entry in data:\n",
    "    tokens = entry[\"tokens\"]\n",
    "    tags = entry[\"tags\"]\n",
    "    \n",
    "    for token, tag in zip(tokens, tags):\n",
    "        processed_data.append({\"tokens\": [token], \"tags\": [tag]})\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(processed_data)\n",
    "\n",
    "# Display the processed dataset\n",
    "print(df.head())\n",
    "\n",
    "# Save the processed data as JSONL file\n",
    "output_file = file_path\n",
    "with open(output_file, \"w\") as f:\n",
    "    for entry in processed_data:\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7e8bb848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         tokens tags\n",
      "0  [Clustering]  [O]\n",
      "1          [of]  [O]\n",
      "2    [missense]  [O]\n",
      "3   [mutations]  [O]\n",
      "4          [in]  [O]\n"
     ]
    }
   ],
   "source": [
    "# Reattempting to process the dataset with one token per label\n",
    "\n",
    "# Load dataset again\n",
    "file_path = \"/media/smartdragon/WORK/6th Semester/22AIE315 - Natural Language Processing/Project/New_Json_Files/combined_test_67.jsonl\"\n",
    "\n",
    "# Read JSONL file into a list\n",
    "data = []\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        data.append(json.loads(line.strip()))\n",
    "\n",
    "# Convert to one token per label\n",
    "processed_data = []\n",
    "for entry in data:\n",
    "    tokens = entry[\"tokens\"]\n",
    "    tags = entry[\"tags\"]\n",
    "    \n",
    "    for token, tag in zip(tokens, tags):\n",
    "        processed_data.append({\"tokens\": [token], \"tags\": [tag]})\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(processed_data)\n",
    "\n",
    "# Display the processed dataset\n",
    "print(df.head())\n",
    "\n",
    "# Save the processed data as JSONL file\n",
    "output_file = file_path\n",
    "with open(output_file, \"w\") as f:\n",
    "    for entry in processed_data:\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7148b55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       tokens tags\n",
      "0     [BRCA1]  [O]\n",
      "1        [is]  [O]\n",
      "2  [secreted]  [O]\n",
      "3       [and]  [O]\n",
      "4  [exhibits]  [O]\n"
     ]
    }
   ],
   "source": [
    "# Reattempting to process the dataset with one token per label\n",
    "\n",
    "# Load dataset again\n",
    "file_path = \"/media/smartdragon/WORK/6th Semester/22AIE315 - Natural Language Processing/Project/New_Json_Files/combined_dev_67.jsonl\"\n",
    "\n",
    "# Read JSONL file into a list\n",
    "data = []\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        data.append(json.loads(line.strip()))\n",
    "\n",
    "# Convert to one token per label\n",
    "processed_data = []\n",
    "for entry in data:\n",
    "    tokens = entry[\"tokens\"]\n",
    "    tags = entry[\"tags\"]\n",
    "    \n",
    "    for token, tag in zip(tokens, tags):\n",
    "        processed_data.append({\"tokens\": [token], \"tags\": [tag]})\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(processed_data)\n",
    "\n",
    "# Display the processed dataset\n",
    "print(df.head())\n",
    "\n",
    "# Save the processed data as JSONL file\n",
    "output_file = file_path\n",
    "with open(output_file, \"w\") as f:\n",
    "    for entry in processed_data:\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1497465e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
