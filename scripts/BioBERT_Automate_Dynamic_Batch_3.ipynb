{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "import numpy as np\n",
    "from seqeval.metrics import classification_report, accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 512  # BERT's max sequence length\n",
    "DATA_DIR = \"/media/smartdragon/WORK/6th Semester/22AIE315 - Natural Language Processing/Project/New_Json_Files/Batch 3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(filepath):\n",
    "    data = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            data.append(json.loads(line.strip()))\n",
    "    return Dataset.from_list(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_long_sentence(tokens, tags, max_len=MAX_LEN - 2):  # -2 for [CLS] and [SEP]\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), max_len):\n",
    "        chunk_tokens = tokens[i:i+max_len]\n",
    "        chunk_tags = tags[i:i+max_len]\n",
    "        chunks.append({'tokens': chunk_tokens, 'tags': chunk_tags})\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(dataset):\n",
    "    split_data = []\n",
    "    for example in dataset:\n",
    "        split_sentences = split_long_sentence(example['tokens'], example['tags'])\n",
    "        split_data.extend(split_sentences)\n",
    "    return Dataset.from_list(split_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(example, tokenizer, label2id):\n",
    "    tokenized = tokenizer(example['tokens'], truncation=True, max_length=512, is_split_into_words=True)\n",
    "    word_ids = tokenized.word_ids()\n",
    "\n",
    "    aligned_labels = []\n",
    "    previous_word_idx = None\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            aligned_labels.append(-100)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            aligned_labels.append(label2id.get(example['tags'][word_idx], -100))\n",
    "        else:\n",
    "            aligned_labels.append(label2id.get(example['tags'][word_idx], -100))\n",
    "\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    tokenized['labels'] = aligned_labels\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p, id2label):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_labels = []\n",
    "    true_predictions = []\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        label_sequence = []\n",
    "        prediction_sequence = []\n",
    "\n",
    "        for j in range(len(labels[i])):\n",
    "            if labels[i][j] != -100:  # Exclude padding tokens\n",
    "                label_sequence.append(id2label.get(labels[i][j], \"O\"))\n",
    "                prediction_sequence.append(id2label.get(predictions[i][j], \"O\"))\n",
    "\n",
    "        true_labels.append(label_sequence)\n",
    "        true_predictions.append(prediction_sequence)\n",
    "\n",
    "    report = classification_report(true_labels, true_predictions, output_dict=True)\n",
    "    accuracy = accuracy_score(true_labels, true_predictions)\n",
    "\n",
    "    return {\n",
    "        \"f1\": report[\"micro avg\"][\"f1-score\"],\n",
    "        \"precision\": report[\"micro avg\"][\"precision\"],\n",
    "        \"recall\": report[\"micro avg\"][\"recall\"],\n",
    "        \"accuracy\": accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_batches():\n",
    "    batch_numbers = set()\n",
    "    for file in os.listdir(DATA_DIR):\n",
    "        if file.startswith(\"combined_train_\") and file.endswith(\".jsonl\"):\n",
    "            batch_num = file.split(\"_\")[-1].replace(\".jsonl\", \"\")\n",
    "            if batch_num.isdigit():\n",
    "                batch_numbers.add(int(batch_num))\n",
    "    return sorted(batch_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(batch_num):\n",
    "    dataset_prefix = f\"combined_{batch_num}\"\n",
    "\n",
    "    train_file = os.path.join(DATA_DIR, f\"combined_train_{batch_num}.jsonl\")\n",
    "    dev_file = os.path.join(DATA_DIR, f\"combined_dev_{batch_num}.jsonl\")\n",
    "    test_file = os.path.join(DATA_DIR, f\"combined_test_{batch_num}.jsonl\")\n",
    "\n",
    "    if not os.path.exists(train_file) or not os.path.exists(dev_file) or not os.path.exists(test_file):\n",
    "        print(f\"🚨 Skipping batch {batch_num}, files not found.\")\n",
    "        return\n",
    "\n",
    "    print(f\"🚀 Processing batch {batch_num}...\")\n",
    "\n",
    "    \n",
    "    model_path = f\"/media/smartdragon/Windows-SSD/Users/sriva/Documents/NLP/BioBERT/biobert-finetuned-batch{batch_num-1}\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    # Load label2id from Batch 1 to ensure consistent label space\n",
    "    with open(os.path.join(model_path, \"label2id.json\"), \"r\") as f:\n",
    "        old_label2id = json.load(f)\n",
    "    id2label = {v: k for k, v in old_label2id.items()}\n",
    "    print(f\"✅ Loaded label2id from Batch {batch_num - 1}: {old_label2id}\")\n",
    "\n",
    "    train_dataset = preprocess_dataset(load_jsonl(train_file))\n",
    "    dev_dataset = preprocess_dataset(load_jsonl(dev_file))\n",
    "    test_dataset = preprocess_dataset(load_jsonl(test_file))\n",
    "    print(f\"After splitting: Train={len(train_dataset)}, Dev={len(dev_dataset)}, Test={len(test_dataset)}\")\n",
    "\n",
    "    unique_tags = set(tag for example in train_dataset['tags'] for tag in example)\n",
    "    label2id = {tag: i for i, tag in enumerate(sorted(unique_tags))}\n",
    "    id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "    train_dataset = train_dataset.map(lambda x: tokenize_and_align_labels(x, tokenizer, label2id))\n",
    "    dev_dataset = dev_dataset.map(lambda x: tokenize_and_align_labels(x, tokenizer, label2id))\n",
    "    test_dataset = test_dataset.map(lambda x: tokenize_and_align_labels(x, tokenizer, label2id))\n",
    "\n",
    "    # Load the number of labels from batch 1\n",
    "    old_num_labels = len(old_label2id)  # Should match previous training setup\n",
    "    new_num_labels = len(label2id)  # Ensure this matches the new dataset\n",
    "\n",
    "    # Load the previously trained model from batch 1\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_path, num_labels=old_num_labels\n",
    "    )\n",
    "\n",
    "    # Extract the old classifier layer\n",
    "    old_classifier = model.classifier\n",
    "\n",
    "    # Create a new classifier layer with updated label count\n",
    "    new_classifier = nn.Linear(old_classifier.in_features, new_num_labels)\n",
    "\n",
    "    # Transfer weights from the old classifier to the new one (for common labels)\n",
    "    with torch.no_grad():\n",
    "        num_common_labels = min(old_num_labels, new_num_labels)\n",
    "        new_classifier.weight[:num_common_labels, :] = old_classifier.weight[:num_common_labels, :]\n",
    "        new_classifier.bias[:num_common_labels] = old_classifier.bias[:num_common_labels]\n",
    "\n",
    "    # Assign the updated classifier to the model\n",
    "    model.classifier = new_classifier\n",
    "\n",
    "    # Save updated model before continuing training\n",
    "    model.save_pretrained(\"/media/smartdragon/Windows-SSD/Users/sriva/Documents/NLP/BioBERT/BioBERT-updated\")\n",
    "    print(f\"✅ Model updated to support new label set while keeping batch {batch_num - 1} training.\")\n",
    "\n",
    "    model_checkpoint = f\"/media/smartdragon/Windows-SSD/Users/sriva/Documents/NLP/BioBERT/biobert-finetuned-batch{batch_num}\"\n",
    "    # Create the directory if it does not exist\n",
    "    os.makedirs(model_checkpoint, exist_ok=True)\n",
    "    print(f\"Directory ensured: {model_checkpoint}\")\n",
    "    model_path_2 = \"/media/smartdragon/Windows-SSD/Users/sriva/Documents/NLP/BioBERT/BioBERT-updated\"\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_path_2, num_labels=len(label2id))\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"BioBERT-finetuned-mtl\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_dir=\"./logs\",\n",
    "        learning_rate=3e-5,\n",
    "        per_device_train_batch_size=64,\n",
    "        per_device_eval_batch_size=64,\n",
    "        num_train_epochs=2,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        greater_is_better=True,\n",
    "        push_to_hub=False,\n",
    "        save_total_limit=2,\n",
    "        gradient_accumulation_steps=8,  # This simulates batch size 4 * 4 = 16\n",
    "        fp16=True,\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=dev_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=lambda p: compute_metrics(p, id2label)\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    metrics = trainer.evaluate(test_dataset)\n",
    "    print(\"Test Set Performance:\")\n",
    "    print(metrics)\n",
    "    \n",
    "    model.save_pretrained(model_checkpoint)\n",
    "    tokenizer.save_pretrained(model_checkpoint)\n",
    "\n",
    "    print(f\"✅ Model trained and saved for batch {batch_num}\")\n",
    "\n",
    "    predictions, labels, _ = trainer.predict(test_dataset)\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "        # Save label2id to file\n",
    "    with open(f\"{model_checkpoint}/label2id.json\", \"w\") as f:\n",
    "        json.dump(label2id, f)\n",
    "    print(\"✅ Saved new_label2id mapping.\")\n",
    "\n",
    "    # Save label2id to file\n",
    "    with open(\"label2id.json\", \"w\") as f:\n",
    "        json.dump(label2id, f)\n",
    "    print(\"✅ Saved new_label2id mapping.\")\n",
    "    print(\"✅ Saved label2id mapping to model directory.\")\n",
    "\n",
    "    # results = classification_report(labels, predictions, output_dict=True)\n",
    "    # with open(f\"{model_checkpoint}_evaluation.json\", \"w\") as f:\n",
    "    #     json.dump(results, f)\n",
    "\n",
    "    # print(f\"📊 Evaluation results saved for batch {batch_num}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Detected batches: [41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67]\n",
      "🚀 Processing batch 41...\n",
      "✅ Loaded label2id from Batch 40: {'B-Cellular_component': 0, 'E-Cellular_component': 1, 'I-Cellular_component': 2, 'O': 3, 'S-Cellular_component': 4}\n",
      "After splitting: Train=69244, Dev=23646, Test=46196\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58e351113c774d5f940a61935bb145ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/69244 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d69629e61d7f463983e1ed57ee41ec7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23646 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fefe6a5a3b6478fb297990369015fa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/46196 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model updated to support new label set while keeping batch 40 training.\n",
      "Directory ensured: /media/smartdragon/Windows-SSD/Users/sriva/Documents/NLP/BioBERT/biobert-finetuned-batch41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/smartdragon/Storage/anaconda3/envs/nlp/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_64089/2071289192.py:92: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='270' max='270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [270/270 03:07, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.109912</td>\n",
       "      <td>0.601660</td>\n",
       "      <td>0.721393</td>\n",
       "      <td>0.516014</td>\n",
       "      <td>0.965845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Performance:\n",
      "{'eval_loss': 0.11798430979251862, 'eval_f1': 0.6156217882836589, 'eval_precision': 0.6575192096597146, 'eval_recall': 0.578743961352657, 'eval_accuracy': 0.9649263602181699, 'eval_runtime': 7.6458, 'eval_samples_per_second': 6041.998, 'eval_steps_per_second': 94.431, 'epoch': 1.990757855822551}\n",
      "✅ Model trained and saved for batch 41\n",
      "✅ Saved new_label2id mapping.\n",
      "✅ Saved new_label2id mapping.\n",
      "✅ Saved label2id mapping to model directory.\n",
      "🚀 Processing batch 42...\n",
      "✅ Loaded label2id from Batch 41: {'B-Simple_chemical': 0, 'I-Simple_chemical': 1, 'O': 2}\n",
      "After splitting: Train=69244, Dev=23646, Test=46196\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36d5b26bd79f46e8b0217c6da9311958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/69244 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2593c85901e8452f91c879aa0e0fb90e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23646 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d61af9a2d8b64f4181730eaf0f558885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/46196 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model updated to support new label set while keeping batch 41 training.\n",
      "Directory ensured: /media/smartdragon/Windows-SSD/Users/sriva/Documents/NLP/BioBERT/biobert-finetuned-batch42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/smartdragon/Storage/anaconda3/envs/nlp/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_64089/2071289192.py:92: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='270' max='270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [270/270 03:04, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.160458</td>\n",
       "      <td>0.654442</td>\n",
       "      <td>0.793712</td>\n",
       "      <td>0.556751</td>\n",
       "      <td>0.961725</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Performance:\n",
      "{'eval_loss': 0.15749002993106842, 'eval_f1': 0.6608748666361836, 'eval_precision': 0.705958971019212, 'eval_recall': 0.6212034383954155, 'eval_accuracy': 0.9592343246095085, 'eval_runtime': 7.6742, 'eval_samples_per_second': 6019.683, 'eval_steps_per_second': 94.082, 'epoch': 1.990757855822551}\n",
      "✅ Model trained and saved for batch 42\n",
      "✅ Saved new_label2id mapping.\n",
      "✅ Saved new_label2id mapping.\n",
      "✅ Saved label2id mapping to model directory.\n",
      "🚀 Processing batch 43...\n",
      "✅ Loaded label2id from Batch 42: {'B-Simple_chemical': 0, 'E-Simple_chemical': 1, 'I-Simple_chemical': 2, 'O': 3, 'S-Simple_chemical': 4}\n",
      "After splitting: Train=69244, Dev=23646, Test=46196\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b988276e22434cedabad85e51a398a74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/69244 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b40f70a87a74e5abd64b3d7dae7eff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23646 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70b3d8f4e391459fba69dcf00e3dc73c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/46196 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model updated to support new label set while keeping batch 42 training.\n",
      "Directory ensured: /media/smartdragon/Windows-SSD/Users/sriva/Documents/NLP/BioBERT/biobert-finetuned-batch43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/smartdragon/Storage/anaconda3/envs/nlp/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_64089/2071289192.py:92: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='270' max='270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [270/270 02:59, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.254831</td>\n",
       "      <td>0.776897</td>\n",
       "      <td>0.781089</td>\n",
       "      <td>0.772749</td>\n",
       "      <td>0.913961</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Performance:\n",
      "{'eval_loss': 0.24930286407470703, 'eval_f1': 0.7937293729372938, 'eval_precision': 0.8241240469459437, 'eval_recall': 0.7654969364207846, 'eval_accuracy': 0.9175026379538396, 'eval_runtime': 7.7264, 'eval_samples_per_second': 5979.011, 'eval_steps_per_second': 93.446, 'epoch': 1.990757855822551}\n",
      "✅ Model trained and saved for batch 43\n",
      "✅ Saved new_label2id mapping.\n",
      "✅ Saved new_label2id mapping.\n",
      "✅ Saved label2id mapping to model directory.\n",
      "🚀 Processing batch 44...\n",
      "✅ Loaded label2id from Batch 43: {'B-Gene_or_gene_product': 0, 'I-Gene_or_gene_product': 1, 'O': 2}\n",
      "After splitting: Train=69244, Dev=23646, Test=46196\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cadb2ffaa614652bb820d508c861a0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/69244 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6baea6f96a01486fa0dad67de814b8a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23646 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "692772325095411fbc3eaf04c5dc36df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/46196 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model updated to support new label set while keeping batch 43 training.\n",
      "Directory ensured: /media/smartdragon/Windows-SSD/Users/sriva/Documents/NLP/BioBERT/biobert-finetuned-batch44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/smartdragon/Storage/anaconda3/envs/nlp/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_64089/2071289192.py:92: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='270' max='270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [270/270 03:21, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.344404</td>\n",
       "      <td>0.774938</td>\n",
       "      <td>0.789895</td>\n",
       "      <td>0.760538</td>\n",
       "      <td>0.899597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Performance:\n",
      "{'eval_loss': 0.3327817916870117, 'eval_f1': 0.7954769315629802, 'eval_precision': 0.8318226763348715, 'eval_recall': 0.7621744054360136, 'eval_accuracy': 0.9008426590574702, 'eval_runtime': 7.7096, 'eval_samples_per_second': 5991.99, 'eval_steps_per_second': 93.649, 'epoch': 1.990757855822551}\n",
      "✅ Model trained and saved for batch 44\n",
      "✅ Saved new_label2id mapping.\n",
      "✅ Saved new_label2id mapping.\n",
      "✅ Saved label2id mapping to model directory.\n",
      "🚀 Processing batch 45...\n",
      "✅ Loaded label2id from Batch 44: {'B-Gene_or_gene_product': 0, 'E-Gene_or_gene_product': 1, 'I-Gene_or_gene_product': 2, 'O': 3, 'S-Gene_or_gene_product': 4}\n",
      "After splitting: Train=69244, Dev=23646, Test=46196\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb283498f919496dbb96e3c8f12a229b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/69244 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "435be28c043b417e9a22cd99f9ec1388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23646 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ea6157ea9454715bf581cf0f2fc5894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/46196 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model updated to support new label set while keeping batch 44 training.\n",
      "Directory ensured: /media/smartdragon/Windows-SSD/Users/sriva/Documents/NLP/BioBERT/biobert-finetuned-batch45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/smartdragon/Storage/anaconda3/envs/nlp/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_64089/2071289192.py:92: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='270' max='270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [270/270 03:23, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.519900</td>\n",
       "      <td>0.710035</td>\n",
       "      <td>0.736795</td>\n",
       "      <td>0.685151</td>\n",
       "      <td>0.872348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Performance:\n",
      "{'eval_loss': 0.4710746705532074, 'eval_f1': 0.7273921971252567, 'eval_precision': 0.7539160045402952, 'eval_recall': 0.7026712509918012, 'eval_accuracy': 0.8764248666161368, 'eval_runtime': 8.1703, 'eval_samples_per_second': 5654.11, 'eval_steps_per_second': 88.368, 'epoch': 1.990757855822551}\n",
      "✅ Model trained and saved for batch 45\n",
      "✅ Saved new_label2id mapping.\n",
      "✅ Saved new_label2id mapping.\n",
      "✅ Saved label2id mapping to model directory.\n",
      "🚀 Processing batch 46...\n",
      "✅ Loaded label2id from Batch 45: {'B-Cellular_component': 0, 'B-Complex': 1, 'B-Gene_or_gene_product': 2, 'B-Simple_chemical': 3, 'I-Cellular_component': 4, 'I-Complex': 5, 'I-Gene_or_gene_product': 6, 'I-Simple_chemical': 7, 'O': 8}\n",
      "After splitting: Train=69244, Dev=23646, Test=46196\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf98d04c3aad4898a39ac4f929bd53db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/69244 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "429b1e1b58ba421787940d0ada5a6e2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23646 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f38fb34dd42e4a96aea8da63228f4bba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/46196 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model updated to support new label set while keeping batch 45 training.\n",
      "Directory ensured: /media/smartdragon/Windows-SSD/Users/sriva/Documents/NLP/BioBERT/biobert-finetuned-batch46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/smartdragon/Storage/anaconda3/envs/nlp/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_64089/2071289192.py:92: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='270' max='270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [270/270 03:05, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.623919</td>\n",
       "      <td>0.728112</td>\n",
       "      <td>0.758161</td>\n",
       "      <td>0.700353</td>\n",
       "      <td>0.855228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Performance:\n",
      "{'eval_loss': 0.5868105888366699, 'eval_f1': 0.7470878079595704, 'eval_precision': 0.7767036200283718, 'eval_recall': 0.7196475513581929, 'eval_accuracy': 0.8534783836402277, 'eval_runtime': 7.8213, 'eval_samples_per_second': 5906.434, 'eval_steps_per_second': 92.312, 'epoch': 1.990757855822551}\n",
      "✅ Model trained and saved for batch 46\n",
      "✅ Saved new_label2id mapping.\n",
      "✅ Saved new_label2id mapping.\n",
      "✅ Saved label2id mapping to model directory.\n",
      "🚀 Processing batch 47...\n",
      "✅ Loaded label2id from Batch 46: {'B-Cellular_component': 0, 'B-Complex': 1, 'B-Gene_or_gene_product': 2, 'B-Simple_chemical': 3, 'E-Cellular_component': 4, 'E-Complex': 5, 'E-Gene_or_gene_product': 6, 'E-Simple_chemical': 7, 'I-Cellular_component': 8, 'I-Complex': 9, 'I-Gene_or_gene_product': 10, 'I-Simple_chemical': 11, 'O': 12, 'S-Cellular_component': 13, 'S-Complex': 14, 'S-Gene_or_gene_product': 15, 'S-Simple_chemical': 16}\n",
      "After splitting: Train=278226, Dev=100166, Test=196904\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbb4666768d14981853cf3779904a40f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/278226 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e13bf6ee85754547a4e3f0cf7feb7b68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100166 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f08bf72e2e164951a11871ba8d6f930a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/196904 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model updated to support new label set while keeping batch 46 training.\n",
      "Directory ensured: /media/smartdragon/Windows-SSD/Users/sriva/Documents/NLP/BioBERT/biobert-finetuned-batch47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/smartdragon/Storage/anaconda3/envs/nlp/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_64089/2071289192.py:92: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1086' max='1086' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1086/1086 07:21, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.030500</td>\n",
       "      <td>0.033178</td>\n",
       "      <td>0.569626</td>\n",
       "      <td>0.573503</td>\n",
       "      <td>0.565801</td>\n",
       "      <td>0.992006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.019800</td>\n",
       "      <td>0.033769</td>\n",
       "      <td>0.585708</td>\n",
       "      <td>0.612305</td>\n",
       "      <td>0.561325</td>\n",
       "      <td>0.992651</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Performance:\n",
      "{'eval_loss': 0.051833271980285645, 'eval_f1': 0.6111702937148075, 'eval_precision': 0.7035739313244569, 'eval_recall': 0.5402206080172182, 'eval_accuracy': 0.9897561345621421, 'eval_runtime': 36.5891, 'eval_samples_per_second': 5381.498, 'eval_steps_per_second': 84.096, 'epoch': 1.999080036798528}\n",
      "✅ Model trained and saved for batch 47\n",
      "✅ Saved new_label2id mapping.\n",
      "✅ Saved new_label2id mapping.\n",
      "✅ Saved label2id mapping to model directory.\n",
      "🚀 Processing batch 48...\n",
      "✅ Loaded label2id from Batch 47: {'B-GO': 0, 'I-GO': 1, 'O': 2}\n",
      "After splitting: Train=278226, Dev=100166, Test=196904\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49c9fdbc713a482ca90b70a8dd47ab6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/278226 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5343afc014ee46cfa2301561c2b6f91d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100166 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b26e9be2c56f4ee28aff40b373abf39d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/196904 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model updated to support new label set while keeping batch 47 training.\n",
      "Directory ensured: /media/smartdragon/Windows-SSD/Users/sriva/Documents/NLP/BioBERT/biobert-finetuned-batch48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/smartdragon/Storage/anaconda3/envs/nlp/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_64089/2071289192.py:92: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1086' max='1086' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1086/1086 07:49, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.044400</td>\n",
       "      <td>0.042073</td>\n",
       "      <td>0.584017</td>\n",
       "      <td>0.595070</td>\n",
       "      <td>0.573367</td>\n",
       "      <td>0.991549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.019700</td>\n",
       "      <td>0.040128</td>\n",
       "      <td>0.596745</td>\n",
       "      <td>0.638916</td>\n",
       "      <td>0.559796</td>\n",
       "      <td>0.992149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Performance:\n",
      "{'eval_loss': 0.06329484283924103, 'eval_f1': 0.6097033374536466, 'eval_precision': 0.7635448916408669, 'eval_recall': 0.5074588477366255, 'eval_accuracy': 0.9886183486934426, 'eval_runtime': 39.7204, 'eval_samples_per_second': 4957.251, 'eval_steps_per_second': 77.466, 'epoch': 1.999080036798528}\n",
      "✅ Model trained and saved for batch 48\n",
      "✅ Saved new_label2id mapping.\n",
      "✅ Saved new_label2id mapping.\n",
      "✅ Saved label2id mapping to model directory.\n",
      "🚀 Processing batch 49...\n",
      "✅ Loaded label2id from Batch 48: {'B-GO': 0, 'E-GO': 1, 'I-GO': 2, 'O': 3, 'S-GO': 4}\n",
      "After splitting: Train=278226, Dev=100166, Test=196904\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4775eab1568949e2ae40abd1167e04c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/278226 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c36b459bbfd3472ab4d9296684f1b5db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100166 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e13c36a75a0469595571f29105bf789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/196904 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model updated to support new label set while keeping batch 48 training.\n",
      "Directory ensured: /media/smartdragon/Windows-SSD/Users/sriva/Documents/NLP/BioBERT/biobert-finetuned-batch49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/smartdragon/Storage/anaconda3/envs/nlp/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_64089/2071289192.py:92: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1086' max='1086' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1086/1086 07:44, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.028300</td>\n",
       "      <td>0.024631</td>\n",
       "      <td>0.779456</td>\n",
       "      <td>0.796843</td>\n",
       "      <td>0.762812</td>\n",
       "      <td>0.992396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.018900</td>\n",
       "      <td>0.025315</td>\n",
       "      <td>0.752212</td>\n",
       "      <td>0.780367</td>\n",
       "      <td>0.726018</td>\n",
       "      <td>0.993146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Performance:\n",
      "{'eval_loss': 0.02628129906952381, 'eval_f1': 0.801029391846133, 'eval_precision': 0.8575986078886311, 'eval_recall': 0.7514612452350699, 'eval_accuracy': 0.9913338643000721, 'eval_runtime': 38.0035, 'eval_samples_per_second': 5181.202, 'eval_steps_per_second': 80.966, 'epoch': 1.999080036798528}\n",
      "✅ Model trained and saved for batch 49\n",
      "✅ Saved new_label2id mapping.\n",
      "✅ Saved new_label2id mapping.\n",
      "✅ Saved label2id mapping to model directory.\n",
      "🚀 Processing batch 50...\n",
      "✅ Loaded label2id from Batch 49: {'B-CL': 0, 'I-CL': 1, 'O': 2}\n",
      "After splitting: Train=278226, Dev=100166, Test=196904\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "640af5491db1473cac79189a9e82a51f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/278226 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c01a7ac0385e4d868911e24643d79c20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100166 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cae7566f971e4b4995bd520e5456ed37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/196904 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model updated to support new label set while keeping batch 49 training.\n",
      "Directory ensured: /media/smartdragon/Windows-SSD/Users/sriva/Documents/NLP/BioBERT/biobert-finetuned-batch50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/smartdragon/Storage/anaconda3/envs/nlp/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_64089/2071289192.py:92: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1086' max='1086' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1086/1086 07:10, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.024100</td>\n",
       "      <td>0.029633</td>\n",
       "      <td>0.777090</td>\n",
       "      <td>0.751497</td>\n",
       "      <td>0.804487</td>\n",
       "      <td>0.992996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>0.030512</td>\n",
       "      <td>0.756884</td>\n",
       "      <td>0.794781</td>\n",
       "      <td>0.722436</td>\n",
       "      <td>0.992809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Performance:\n",
      "{'eval_loss': 0.030288292095065117, 'eval_f1': 0.846243072560897, 'eval_precision': 0.8806330472103004, 'eval_recall': 0.8144381046886628, 'eval_accuracy': 0.9930898471574316, 'eval_runtime': 36.6137, 'eval_samples_per_second': 5377.872, 'eval_steps_per_second': 84.039, 'epoch': 1.999080036798528}\n",
      "✅ Model trained and saved for batch 50\n",
      "✅ Saved new_label2id mapping.\n",
      "✅ Saved new_label2id mapping.\n",
      "✅ Saved label2id mapping to model directory.\n",
      "🚀 Processing batch 51...\n",
      "✅ Loaded label2id from Batch 50: {'B-CL': 0, 'E-CL': 1, 'I-CL': 2, 'O': 3, 'S-CL': 4}\n",
      "After splitting: Train=278226, Dev=100166, Test=196904\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddfe67b6456c4f49952513ebdadb89e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/278226 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7680a165156a45599db694d1bbcdbb79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100166 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41a49f3104464f99b2501e159dc2cdae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/196904 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model updated to support new label set while keeping batch 50 training.\n",
      "Directory ensured: /media/smartdragon/Windows-SSD/Users/sriva/Documents/NLP/BioBERT/biobert-finetuned-batch51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/smartdragon/Storage/anaconda3/envs/nlp/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_64089/2071289192.py:92: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1086' max='1086' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1086/1086 07:13, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.032000</td>\n",
       "      <td>0.030538</td>\n",
       "      <td>0.829636</td>\n",
       "      <td>0.838764</td>\n",
       "      <td>0.820704</td>\n",
       "      <td>0.993678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.015200</td>\n",
       "      <td>0.030882</td>\n",
       "      <td>0.825773</td>\n",
       "      <td>0.877095</td>\n",
       "      <td>0.780124</td>\n",
       "      <td>0.993836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Performance:\n",
      "{'eval_loss': 0.03280822932720184, 'eval_f1': 0.7859929769801015, 'eval_precision': 0.7697745510126098, 'eval_recall': 0.8029095257074532, 'eval_accuracy': 0.9914514355065044, 'eval_runtime': 34.7911, 'eval_samples_per_second': 5659.611, 'eval_steps_per_second': 88.442, 'epoch': 1.999080036798528}\n",
      "✅ Model trained and saved for batch 51\n",
      "✅ Saved new_label2id mapping.\n",
      "✅ Saved new_label2id mapping.\n",
      "✅ Saved label2id mapping to model directory.\n",
      "🚀 Processing batch 52...\n",
      "✅ Loaded label2id from Batch 51: {'B-CHEBI': 0, 'I-CHEBI': 1, 'O': 2}\n",
      "After splitting: Train=278226, Dev=100166, Test=196904\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2ccc49ffbf24913bc2bde41345a319d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/278226 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79a6e0a8c72c449ba618bfbf4f779003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100166 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "283960fa8c2c4ca1b6108c745121327c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/196904 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model updated to support new label set while keeping batch 51 training.\n",
      "Directory ensured: /media/smartdragon/Windows-SSD/Users/sriva/Documents/NLP/BioBERT/biobert-finetuned-batch52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/smartdragon/Storage/anaconda3/envs/nlp/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_64089/2071289192.py:92: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1086' max='1086' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1086/1086 07:20, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.027300</td>\n",
       "      <td>0.041521</td>\n",
       "      <td>0.831369</td>\n",
       "      <td>0.893597</td>\n",
       "      <td>0.777244</td>\n",
       "      <td>0.993214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.016600</td>\n",
       "      <td>0.038294</td>\n",
       "      <td>0.834160</td>\n",
       "      <td>0.903316</td>\n",
       "      <td>0.774840</td>\n",
       "      <td>0.993139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Performance:\n",
      "{'eval_loss': 0.037297721952199936, 'eval_f1': 0.8160016539177176, 'eval_precision': 0.8713024282560706, 'eval_recall': 0.7673017107309487, 'eval_accuracy': 0.9920392915386658, 'eval_runtime': 36.7866, 'eval_samples_per_second': 5352.606, 'eval_steps_per_second': 83.645, 'epoch': 1.999080036798528}\n",
      "✅ Model trained and saved for batch 52\n",
      "✅ Saved new_label2id mapping.\n",
      "✅ Saved new_label2id mapping.\n",
      "✅ Saved label2id mapping to model directory.\n",
      "🚀 Processing batch 53...\n",
      "✅ Loaded label2id from Batch 52: {'B-CHEBI': 0, 'E-CHEBI': 1, 'I-CHEBI': 2, 'O': 3, 'S-CHEBI': 4}\n",
      "After splitting: Train=278226, Dev=100166, Test=196904\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06b03a3fe6da4b99962709a8b19d4b0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/278226 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3adf687677d8476eac3e977f730f5382",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100166 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beb4e1833c2e48bfa45bc9508da38808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/196904 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model updated to support new label set while keeping batch 52 training.\n",
      "Directory ensured: /media/smartdragon/Windows-SSD/Users/sriva/Documents/NLP/BioBERT/biobert-finetuned-batch53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/smartdragon/Storage/anaconda3/envs/nlp/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_64089/2071289192.py:92: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1086' max='1086' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1086/1086 07:20, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.088000</td>\n",
       "      <td>0.151327</td>\n",
       "      <td>0.663435</td>\n",
       "      <td>0.773988</td>\n",
       "      <td>0.580517</td>\n",
       "      <td>0.964223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.057800</td>\n",
       "      <td>0.168290</td>\n",
       "      <td>0.668959</td>\n",
       "      <td>0.767055</td>\n",
       "      <td>0.593108</td>\n",
       "      <td>0.964575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Performance:\n",
      "{'eval_loss': 0.19940562546253204, 'eval_f1': 0.559775938571923, 'eval_precision': 0.6851552268700408, 'eval_recall': 0.47318578420940627, 'eval_accuracy': 0.9440929950316683, 'eval_runtime': 35.0524, 'eval_samples_per_second': 5617.417, 'eval_steps_per_second': 87.783, 'epoch': 1.999080036798528}\n",
      "✅ Model trained and saved for batch 53\n",
      "✅ Saved new_label2id mapping.\n",
      "✅ Saved new_label2id mapping.\n",
      "✅ Saved label2id mapping to model directory.\n",
      "🚀 Processing batch 54...\n",
      "✅ Loaded label2id from Batch 53: {'B-GGP': 0, 'I-GGP': 1, 'O': 2}\n",
      "After splitting: Train=278226, Dev=100166, Test=196904\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d23309c9d92403aa8c01dbf09d70b57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/278226 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfbd84c3fffe438fbf2961fd6b08acb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100166 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0997d33cae24bda9c8333d6e40135d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/196904 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model updated to support new label set while keeping batch 53 training.\n",
      "Directory ensured: /media/smartdragon/Windows-SSD/Users/sriva/Documents/NLP/BioBERT/biobert-finetuned-batch54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/smartdragon/Storage/anaconda3/envs/nlp/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_64089/2071289192.py:92: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1086' max='1086' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1086/1086 07:12, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.066500</td>\n",
       "      <td>0.219520</td>\n",
       "      <td>0.702198</td>\n",
       "      <td>0.818961</td>\n",
       "      <td>0.614576</td>\n",
       "      <td>0.964418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.054600</td>\n",
       "      <td>0.215806</td>\n",
       "      <td>0.690932</td>\n",
       "      <td>0.809880</td>\n",
       "      <td>0.602450</td>\n",
       "      <td>0.963121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Performance:\n",
      "{'eval_loss': 0.30478954315185547, 'eval_f1': 0.5424990456801119, 'eval_precision': 0.7023887973640857, 'eval_recall': 0.44190505804311775, 'eval_accuracy': 0.9378389653733834, 'eval_runtime': 36.8081, 'eval_samples_per_second': 5349.472, 'eval_steps_per_second': 83.596, 'epoch': 1.999080036798528}\n",
      "✅ Model trained and saved for batch 54\n",
      "✅ Saved new_label2id mapping.\n",
      "✅ Saved new_label2id mapping.\n",
      "✅ Saved label2id mapping to model directory.\n",
      "🚀 Processing batch 55...\n",
      "✅ Loaded label2id from Batch 54: {'B-GGP': 0, 'E-GGP': 1, 'I-GGP': 2, 'O': 3, 'S-GGP': 4}\n",
      "After splitting: Train=278226, Dev=100166, Test=196904\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb8009336e6b4f539144e5a207c1b1a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/278226 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "914991502ef1473294e0a58bb7a14192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100166 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e251a4893df47de85e17f11a2b8e4e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/196904 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model updated to support new label set while keeping batch 54 training.\n",
      "Directory ensured: /media/smartdragon/Windows-SSD/Users/sriva/Documents/NLP/BioBERT/biobert-finetuned-batch55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/smartdragon/Storage/anaconda3/envs/nlp/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_64089/2071289192.py:92: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1086' max='1086' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1086/1086 07:15, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.228500</td>\n",
       "      <td>0.396327</td>\n",
       "      <td>0.742980</td>\n",
       "      <td>0.772982</td>\n",
       "      <td>0.715219</td>\n",
       "      <td>0.919867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.138200</td>\n",
       "      <td>0.366279</td>\n",
       "      <td>0.751679</td>\n",
       "      <td>0.791374</td>\n",
       "      <td>0.715775</td>\n",
       "      <td>0.923692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Performance:\n",
      "{'eval_loss': 0.41799476742744446, 'eval_f1': 0.6895043731778425, 'eval_precision': 0.7589046956893786, 'eval_recall': 0.6317335945151812, 'eval_accuracy': 0.9036788409754618, 'eval_runtime': 43.9143, 'eval_samples_per_second': 4483.82, 'eval_steps_per_second': 70.068, 'epoch': 1.999080036798528}\n",
      "✅ Model trained and saved for batch 55\n",
      "✅ Saved new_label2id mapping.\n",
      "✅ Saved new_label2id mapping.\n",
      "✅ Saved label2id mapping to model directory.\n",
      "🚀 Processing batch 56...\n",
      "✅ Loaded label2id from Batch 55: {'B-CHEBI': 0, 'B-CL': 1, 'B-GGP': 2, 'B-GO': 3, 'B-SO': 4, 'B-Taxon': 5, 'I-CHEBI': 6, 'I-CL': 7, 'I-GGP': 8, 'I-GO': 9, 'I-SO': 10, 'I-Taxon': 11, 'O': 12}\n",
      "After splitting: Train=278226, Dev=100166, Test=196904\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bea59afd05b48d9b1d80327ab7ae567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/278226 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f428b056a1d7438e82ae0b7db68b5530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100166 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d00716696e944f4ba178d57189734db6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/196904 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model updated to support new label set while keeping batch 55 training.\n",
      "Directory ensured: /media/smartdragon/Windows-SSD/Users/sriva/Documents/NLP/BioBERT/biobert-finetuned-batch56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/smartdragon/Storage/anaconda3/envs/nlp/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_64089/2071289192.py:92: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1086' max='1086' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1086/1086 08:30, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.219400</td>\n",
       "      <td>0.485998</td>\n",
       "      <td>0.771259</td>\n",
       "      <td>0.834147</td>\n",
       "      <td>0.717189</td>\n",
       "      <td>0.920730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.138900</td>\n",
       "      <td>0.449271</td>\n",
       "      <td>0.771028</td>\n",
       "      <td>0.820402</td>\n",
       "      <td>0.727261</td>\n",
       "      <td>0.920137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Performance:\n",
      "{'eval_loss': 0.5946175456047058, 'eval_f1': 0.7040060248281225, 'eval_precision': 0.7973530706581554, 'eval_recall': 0.6302248706015398, 'eval_accuracy': 0.8964121818940342, 'eval_runtime': 72.9964, 'eval_samples_per_second': 2697.448, 'eval_steps_per_second': 42.153, 'epoch': 1.999080036798528}\n",
      "✅ Model trained and saved for batch 56\n",
      "✅ Saved new_label2id mapping.\n",
      "✅ Saved new_label2id mapping.\n",
      "✅ Saved label2id mapping to model directory.\n",
      "🚀 Processing batch 57...\n",
      "✅ Loaded label2id from Batch 56: {'B-CHEBI': 0, 'B-CL': 1, 'B-GGP': 2, 'B-GO': 3, 'B-SO': 4, 'B-Taxon': 5, 'E-CHEBI': 6, 'E-CL': 7, 'E-GGP': 8, 'E-GO': 9, 'E-SO': 10, 'E-Taxon': 11, 'I-CHEBI': 12, 'I-CL': 13, 'I-GGP': 14, 'I-GO': 15, 'I-SO': 16, 'I-Taxon': 17, 'O': 18, 'S-CHEBI': 19, 'S-CL': 20, 'S-GGP': 21, 'S-GO': 22, 'S-SO': 23, 'S-Taxon': 24}\n",
      "After splitting: Train=278226, Dev=100166, Test=196904\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d14e90bd61a4038a028f03fd3964466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/278226 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af333422b9db41a58dba094dde1a1e90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100166 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c0648d36000474a808d562a739a1185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/196904 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model updated to support new label set while keeping batch 56 training.\n",
      "Directory ensured: /media/smartdragon/Windows-SSD/Users/sriva/Documents/NLP/BioBERT/biobert-finetuned-batch57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/smartdragon/Storage/anaconda3/envs/nlp/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_64089/2071289192.py:92: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 5] Input/output error: 'BioBERT-finetuned-mtl'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m📝 Detected batches: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_numbers\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch_num \u001b[38;5;129;01min\u001b[39;00m batch_numbers:\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_num\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m🎉 All detected batches processed successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 92\u001b[39m, in \u001b[36mtrain_and_evaluate\u001b[39m\u001b[34m(batch_num)\u001b[39m\n\u001b[32m     71\u001b[39m training_args = TrainingArguments(\n\u001b[32m     72\u001b[39m     output_dir=\u001b[33m\"\u001b[39m\u001b[33mBioBERT-finetuned-mtl\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     73\u001b[39m     evaluation_strategy=\u001b[33m\"\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     87\u001b[39m     fp16=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     88\u001b[39m )\n\u001b[32m     90\u001b[39m data_collator = DataCollatorForTokenClassification(tokenizer)\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m trainer = \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdev_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mid2label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m trainer.train()\n\u001b[32m    104\u001b[39m metrics = trainer.evaluate(test_dataset)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/smartdragon/Storage/anaconda3/envs/nlp/lib/python3.12/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/smartdragon/Storage/anaconda3/envs/nlp/lib/python3.12/site-packages/transformers/trainer.py:690\u001b[39m, in \u001b[36mTrainer.__init__\u001b[39m\u001b[34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics)\u001b[39m\n\u001b[32m    688\u001b[39m     \u001b[38;5;28mself\u001b[39m.init_hf_repo()\n\u001b[32m    689\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.should_save:\n\u001b[32m--> \u001b[39m\u001b[32m690\u001b[39m     \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    692\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m.data_collator) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.data_collator, \u001b[33m\"\u001b[39m\u001b[33mcollate_batch\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)):\n\u001b[32m    693\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mThe `data_collator` should be a simple callable (function, class with `__call__`).\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen os>:225\u001b[39m, in \u001b[36mmakedirs\u001b[39m\u001b[34m(name, mode, exist_ok)\u001b[39m\n",
      "\u001b[31mOSError\u001b[39m: [Errno 5] Input/output error: 'BioBERT-finetuned-mtl'"
     ]
    }
   ],
   "source": [
    "batch_numbers = detect_batches()\n",
    "print(f\"📝 Detected batches: {batch_numbers}\")\n",
    "\n",
    "for batch_num in batch_numbers:\n",
    "    train_and_evaluate(batch_num)\n",
    "\n",
    "print(\"🎉 All detected batches processed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
